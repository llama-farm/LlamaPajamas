[project]
name = "llama-pajamas-quant"
version = "0.1.0"
description = "Architecture-aware LLM quantization pipeline for GGUF and MLX formats"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "transformers>=4.45.0",
    "torch>=2.4.0",
    "huggingface-hub>=0.24.0",
    "safetensors>=0.4.0",
    "datasets>=2.20.0",
    "sentencepiece>=0.2.0",
    "protobuf>=4.25.0",
    "mlx>=0.19.0",
    "mlx-lm>=0.19.0",
    "llama-cpp-python>=0.3.16",
    "psutil>=7.1.3",
    "openai>=2.7.1",
    "anthropic>=0.39.0",
    "python-dotenv>=1.2.1",
    "accelerate>=1.11.0",
    "lm-eval[api]>=0.4.9",
    "tenacity>=9.1.2",
    "onnx>=1.15.0",
    "onnxruntime>=1.17.0",
    "optimum[onnxruntime]>=1.16.0",
    "onnxconverter-common>=1.14.0",
    "onnxscript>=0.5.0",
    # "onnxsim>=0.4.0",  # Optional bloat reduction (C++ build issues on macOS)
    "ultralytics>=8.0.0",  # For YOLO export
    "coremltools>=7.0.0",  # For CoreML export
    "openai-whisper>=20230314",  # For Whisper export
    "librosa>=0.10.0",  # For audio processing
    "soundfile>=0.12.1",  # For audio I/O
    "pillow>=10.0.0",  # For image processing
]

[project.scripts]
llama-pajamas-quant = "llama_pajamas_quant.cli.main:main"
lp-quant = "llama_pajamas_quant.cli.main:main"
lp-export = "llama_pajamas_quant.exporters.unified:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
